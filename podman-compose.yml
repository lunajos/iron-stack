version: "3.9"

networks:
  iron-stack-net:
    driver: bridge

services:
  # ---------- PostgreSQL ----------
  postgres:
    image: "docker.io/postgres:${POSTGRES_VERSION}-alpine"
    container_name: postgres
    networks:
      - iron-stack-net
    environment:
      POSTGRES_USER: "${POSTGRES_USER}"
      POSTGRES_PASSWORD: "${POSTGRES_PASSWORD}"
      POSTGRES_DB: "${POSTGRES_DB}"
    ports: ["15432:5432"]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB} -h 127.0.0.1"]
      interval: 3s
      timeout: 2s
      retries: 20
    # ephemeral by default (no volume)
    profiles: ["", "persist"]

  postgres_persist:
    image: "docker.io/postgres:${POSTGRES_VERSION}-alpine"
    container_name: postgres
    networks:
      - iron-stack-net
    environment:
      POSTGRES_USER: "${POSTGRES_USER}"
      POSTGRES_PASSWORD: "${POSTGRES_PASSWORD}"
      POSTGRES_DB: "${POSTGRES_DB}"
    ports: ["15432:5432"]
    volumes:
      - ./data/postgres:/var/lib/postgresql/data:Z
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB} -h 127.0.0.1"]
      interval: 3s
      timeout: 2s
      retries: 20
    profiles: ["persist"]

  # ---------- Valkey ----------
  valkey:
    image: "${VALKEY_IMAGE}"
    container_name: valkey
    networks:
      - iron-stack-net
    command: ["valkey-server", "/etc/valkey/valkey.conf"]
    ports: ["6379:6379"]
    volumes:
      - type: bind
        source: ./valkey.conf
        target: /etc/valkey/valkey.conf
        bind: { selinux: "Z" }
    healthcheck:
      test: ["CMD", "valkey-cli", "ping"]
      interval: 2s
      timeout: 1s
      retries: 30
    profiles: ["", "persist"]

  valkey_persist:
    image: "${VALKEY_IMAGE}"
    container_name: valkey
    networks:
      - iron-stack-net
    command: ["valkey-server", "/etc/valkey/valkey.conf"]
    ports: ["6379:6379"]
    volumes:
      - ./valkey.conf:/etc/valkey/valkey.conf:Z
      - ./data/valkey:/data:Z
    healthcheck:
      test: ["CMD", "valkey-cli", "ping"]
      interval: 2s
      timeout: 1s
      retries: 30
    profiles: ["persist"]

  # ---------- Keycloak ----------
  keycloak:
    image: "quay.io/keycloak/keycloak:${KEYCLOAK_VERSION}"
    container_name: keycloak
    networks:
      - iron-stack-net
    command: ["start-dev"]
    depends_on:
      postgres: { condition: service_healthy }
    environment:
      KC_DB: postgres
      KC_DB_URL_HOST: postgres
      KC_DB_URL_DATABASE: "${POSTGRES_DB}"
      KC_DB_USERNAME: "${POSTGRES_USER}"
      KC_DB_PASSWORD: "${POSTGRES_PASSWORD}"
      KC_HTTP_PORT: "${KC_HTTP_PORT}"
      KEYCLOAK_ADMIN: "${KEYCLOAK_ADMIN}"
      KEYCLOAK_ADMIN_PASSWORD: "${KEYCLOAK_ADMIN_PASSWORD}"
      KC_HOSTNAME: "localhost"
    ports: ["${KC_HTTP_PORT}:${KC_HTTP_PORT}"]
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:${KC_HTTP_PORT}/health/ready || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 30
    profiles: ["", "persist"]

  keycloak_persist:
    image: "quay.io/keycloak/keycloak:${KEYCLOAK_VERSION}"
    container_name: keycloak
    networks:
      - iron-stack-net
    command: ["start-dev"]
    depends_on:
      postgres_persist: { condition: service_started }
    environment:
      KC_DB: postgres
      KC_DB_URL_HOST: postgres
      KC_DB_URL_DATABASE: "${POSTGRES_DB}"
      KC_DB_USERNAME: "${POSTGRES_USER}"
      KC_DB_PASSWORD: "${POSTGRES_PASSWORD}"
      KC_HTTP_PORT: "${KC_HTTP_PORT}"
      KEYCLOAK_ADMIN: "${KEYCLOAK_ADMIN}"
      KEYCLOAK_ADMIN_PASSWORD: "${KEYCLOAK_ADMIN_PASSWORD}"
      KC_HOSTNAME: "localhost"
    ports: ["${KC_HTTP_PORT}:${KC_HTTP_PORT}"]
    volumes:
      - ./data/keycloak:/opt/keycloak/data:Z
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:${KC_HTTP_PORT}/health/ready || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 30
    profiles: ["persist"]

  # ---------- Elasticsearch ----------
  es:
    image: "docker.elastic.co/elasticsearch/elasticsearch:${ES_VERSION}"
    container_name: es
    networks:
      - iron-stack-net
    environment:
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - ES_JAVA_OPTS=-Xms${ES_JAVA_MEM} -Xmx${ES_JAVA_MEM}
      - xpack.security.enabled=true
      - discovery.type=single-node
      - xpack.license.self_generated.type=basic
    ulimits:
      memlock: { soft: -1, hard: -1 }
      nofile:  { soft: 65536, hard: 65536 }
    ports: ["9200:9200"]
    volumes:
      - ./data/elasticsearch:/usr/share/elasticsearch/data:Z
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS -u elastic:${ELASTIC_PASSWORD} http://127.0.0.1:9200 >/dev/null || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 40
    profiles: ["", "persist"]
    
  # ---------- Kibana ----------
  kibana:
    image: "docker.elastic.co/kibana/kibana:${ES_VERSION}"
    container_name: kibana
    networks:
      - iron-stack-net
    depends_on:
      es: { condition: service_healthy }
    environment:
      - ELASTICSEARCH_HOSTS=http://es:9200
      - ELASTICSEARCH_SERVICEACCOUNTTOKEN=${FLEET_SERVER_SERVICE_TOKEN}
      - XPACK_SECURITY_ENCRYPTIONKEY=${KIBANA_ENCRYPTION_KEY}
      - XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY=${KIBANA_ENCRYPTION_KEY}
      - XPACK_REPORTING_ENCRYPTIONKEY=${KIBANA_ENCRYPTION_KEY}
      - XPACK_FLEET_ENABLED=true
    ports: ["5601:5601"]
    volumes:
      - ./data/kibana:/usr/share/kibana/data:Z
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:5601/api/status || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 20
    profiles: ["", "persist"]

  es_persist:
    image: "docker.elastic.co/elasticsearch/elasticsearch:${ES_VERSION}"
    container_name: es
    networks:
      - iron-stack-net
    environment:
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - ES_JAVA_OPTS=-Xms${ES_JAVA_MEM} -Xmx${ES_JAVA_MEM}
      - xpack.security.enabled=true
      - discovery.type=single-node
      - xpack.license.self_generated.type=basic
    ulimits:
      memlock: { soft: -1, hard: -1 }
      nofile:  { soft: 65536, hard: 65536 }
    ports: ["9200:9200"]
    volumes:
      - ./data/elasticsearch:/usr/share/elasticsearch/data:Z
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS -u elastic:${ELASTIC_PASSWORD} http://127.0.0.1:9200 >/dev/null || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 40
    profiles: ["persist"]
    
  kibana_persist:
    image: "docker.elastic.co/kibana/kibana:${ES_VERSION}"
    container_name: kibana
    networks:
      - iron-stack-net
    depends_on:
      es_persist: { condition: service_healthy }
    environment:
      - ELASTICSEARCH_HOSTS=http://es:9200
      - ELASTICSEARCH_SERVICEACCOUNTTOKEN=${FLEET_SERVER_SERVICE_TOKEN}
      - XPACK_SECURITY_ENCRYPTIONKEY=${KIBANA_ENCRYPTION_KEY}
      - XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY=${KIBANA_ENCRYPTION_KEY}
      - XPACK_REPORTING_ENCRYPTIONKEY=${KIBANA_ENCRYPTION_KEY}
      - XPACK_FLEET_ENABLED=true
    ports: ["5601:5601"]
    volumes:
      - ./data/kibana:/usr/share/kibana/data:Z
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:5601/api/status || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 20
    profiles: ["persist"]

  # ---------- Prometheus (ALWAYS persistent) ----------
  prometheus:
    image: "docker.io/prom/prometheus:v${PROM_VERSION}"
    container_name: prometheus
    networks:
      - iron-stack-net
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.enable-lifecycle"
    ports: ["9090:9090"]
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:Z
      - ./data/prometheus:/prometheus:Z
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:9090/-/ready >/dev/null || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 30

  # ---------- Grafana (ALWAYS persistent) ----------
  grafana:
    image: "docker.io/grafana/grafana:${GRAFANA_VERSION}"
    container_name: grafana
    networks:
      - iron-stack-net
    environment:
      GF_SECURITY_ADMIN_USER: "${GRAFANA_ADMIN_USER}"
      GF_SECURITY_ADMIN_PASSWORD: "${GRAFANA_ADMIN_PASSWORD}"
      GF_INSTALL_PLUGINS: ""
    depends_on:
      prometheus: { condition: service_started }
    ports: ["3000:3000"]
    volumes:
      - ./data/grafana:/var/lib/grafana:Z
      - ./provisioning/grafana/datasources:/etc/grafana/provisioning/datasources:Z
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:3000/api/health | grep -q okay || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 30

  # ---------- MinIO (Object Storage) ----------
  minio:
    image: "quay.io/minio/minio:latest"
    container_name: minio
    networks:
      - iron-stack-net
    command: ["server", "/data", "--console-address", ":9001"]
    environment:
      MINIO_ROOT_USER: "minioadmin"
      MINIO_ROOT_PASSWORD: "minioadmin"
    ports: 
      - "9000:9000"
      - "9001:9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 5s
      timeout: 3s
      retries: 30
    profiles: ["", "persist"]

  minio_persist:
    image: "quay.io/minio/minio:latest"
    container_name: minio
    networks:
      - iron-stack-net
    command: ["server", "/data", "--console-address", ":9001"]
    environment:
      MINIO_ROOT_USER: "minioadmin"
      MINIO_ROOT_PASSWORD: "minioadmin"
    ports: 
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ./data/minio:/data:Z
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 5s
      timeout: 3s
      retries: 30
    profiles: ["persist"]

  # ---------- Mailpit (Email Testing) ----------
  mailpit:
    image: "axllent/mailpit:latest"
    container_name: mailpit
    networks:
      - iron-stack-net
    ports:
      - "8025:8025"
      - "1025:1025"
    volumes:
      - ./data/mailpit:/data:Z
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8025/api/v1/health"]
      interval: 5s
      timeout: 3s
      retries: 30
    profiles: ["", "persist"]

  # ---------- Node Exporter (System Metrics) ----------
  node_exporter:
    image: "quay.io/prometheus/node-exporter:latest"
    container_name: node-exporter
    networks:
      - iron-stack-net
    command: ["--path.rootfs=/host"]
    ports:
      - "9100:9100"
    volumes:
      - "/:/host:ro,rslave"
    pid: "host"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9100/metrics"]
      interval: 5s
      timeout: 3s
      retries: 30
    profiles: ["", "persist"]

  # ---------- Alertmanager (Alert Handling) ----------
  alertmanager:
    image: "quay.io/prometheus/alertmanager:latest"
    container_name: alertmanager
    networks:
      - iron-stack-net
    ports:
      - "9093:9093"
    volumes:
      - ./alertmanager.yml:/etc/alertmanager/config.yml:Z
      - ./data/alertmanager:/alertmanager:Z
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9093/-/healthy"]
      interval: 5s
      timeout: 3s
      retries: 30
    profiles: ["", "persist"]

  # ---------- Zot (Container Registry) ----------
  zot:
    image: "ghcr.io/project-zot/zot-linux-amd64:latest"
    container_name: zot
    networks:
      - iron-stack-net
    ports:
      - "5000:5000"
    volumes:
      - ./zot-config.json:/etc/zot/config.json:Z
      - ./data/zot:/var/lib/registry:Z
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/v2/"]
      interval: 5s
      timeout: 3s
      retries: 30
    profiles: ["", "persist"]
    
  # ---------- Registry UI (Zot Frontend) ----------
  registry-ui:
    image: "joxit/docker-registry-ui:latest"
    container_name: registry-ui
    networks:
      - iron-stack-net
    ports:
      - "8081:80"
    environment:
      REGISTRY_URL: "http://zot:5000"
      DELETE_IMAGES: "true"
    depends_on:
      zot: { condition: service_started }
    profiles: ["", "persist"]

  # ---------- Metricbeat (Metrics Collector) ----------
  metricbeat:
    image: "docker.elastic.co/beats/metricbeat:8.14.3"
    container_name: metricbeat
    networks:
      - iron-stack-net
    user: root
    volumes:
      - ./config/metricbeat/metricbeat.yml:/usr/share/metricbeat/metricbeat.yml:ro
      - ./config/elasticsearch/certs/ca/ca.crt:/usr/share/metricbeat/ca.crt:ro,Z
    environment:
      - ELASTICSEARCH_HOSTS=https://es:9200
      - ELASTICSEARCH_USERNAME=elastic
      - ELASTICSEARCH_PASSWORD=${ELASTIC_PASSWORD}
      - ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES=/usr/share/metricbeat/ca.crt
    depends_on:
      es: { condition: service_healthy }
      prometheus: { condition: service_started }
    profiles: ["", "persist"]
    
  # ---------- Fleet Server ----------
  fleet-server:
    image: "docker.elastic.co/beats/elastic-agent:${ES_VERSION}"
    container_name: fleet-server
    networks:
      - iron-stack-net
    ports:
      - "8220:8220"
    depends_on:
      kibana: { condition: service_healthy }
    environment:
      - FLEET_SERVER_ENABLE=true
      - FLEET_SERVER_ELASTICSEARCH_HOST=http://es:9200
      - FLEET_SERVER_ELASTICSEARCH_USERNAME=elastic
      - FLEET_SERVER_ELASTICSEARCH_PASSWORD=${ELASTIC_PASSWORD}
      # The setup script will create this token
      - FLEET_SERVER_SERVICE_TOKEN=${FLEET_SERVER_SERVICE_TOKEN}
      - FLEET_SERVER_POLICY_ID=fleet-server-policy
      - FLEET_URL=http://fleet-server:8220
      - KIBANA_FLEET_SETUP=true
      - KIBANA_HOST=http://kibana:5601
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:8220 >/dev/null || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 40
    profiles: ["", "persist"]
