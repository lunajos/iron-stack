version: "3.9"

networks:
  iron-stack-net:
    driver: bridge

services:
  # ---------- PostgreSQL ----------
  postgres:
    image: "docker.io/postgres:${POSTGRES_VERSION}-alpine"
    container_name: postgres
    networks:
      - iron-stack-net
    environment:
      POSTGRES_USER: "${POSTGRES_USER}"
      POSTGRES_PASSWORD: "${POSTGRES_PASSWORD}"
      POSTGRES_DB: "${POSTGRES_DB}"
    ports: ["15432:5432"]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB} -h 127.0.0.1"]
      interval: 3s
      timeout: 2s
      retries: 20
    # ephemeral by default (no volume)
    profiles: ["", "persist"]

  postgres_persist:
    image: "docker.io/postgres:${POSTGRES_VERSION}-alpine"
    container_name: postgres
    networks:
      - iron-stack-net
    environment:
      POSTGRES_USER: "${POSTGRES_USER}"
      POSTGRES_PASSWORD: "${POSTGRES_PASSWORD}"
      POSTGRES_DB: "${POSTGRES_DB}"
    ports: ["15432:5432"]
    volumes:
      - ./data/postgres:/var/lib/postgresql/data:Z
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB} -h 127.0.0.1"]
      interval: 3s
      timeout: 2s
      retries: 20
    profiles: ["persist"]

  # ---------- Valkey ----------
  valkey:
    image: "${VALKEY_IMAGE}"
    container_name: valkey
    networks:
      - iron-stack-net
    command: ["valkey-server", "/etc/valkey/valkey.conf"]
    ports: ["6379:6379"]
    volumes:
      - type: bind
        source: ./valkey.conf
        target: /etc/valkey/valkey.conf
        bind: { selinux: "Z" }
    healthcheck:
      test: ["CMD", "valkey-cli", "ping"]
      interval: 2s
      timeout: 1s
      retries: 30
    profiles: ["", "persist"]

  valkey_persist:
    image: "${VALKEY_IMAGE}"
    container_name: valkey
    networks:
      - iron-stack-net
    command: ["valkey-server", "/etc/valkey/valkey.conf"]
    ports: ["6379:6379"]
    volumes:
      - ./valkey.conf:/etc/valkey/valkey.conf:Z
      - ./data/valkey:/data:Z
    healthcheck:
      test: ["CMD", "valkey-cli", "ping"]
      interval: 2s
      timeout: 1s
      retries: 30
    profiles: ["persist"]

  # ---------- Keycloak ----------
  keycloak:
    image: "quay.io/keycloak/keycloak:${KEYCLOAK_VERSION}"
    container_name: keycloak
    networks:
      - iron-stack-net
    command: ["start-dev"]
    depends_on:
      postgres: { condition: service_healthy }
    environment:
      KC_DB: postgres
      KC_DB_URL_HOST: postgres
      KC_DB_URL_DATABASE: "${POSTGRES_DB}"
      KC_DB_USERNAME: "${POSTGRES_USER}"
      KC_DB_PASSWORD: "${POSTGRES_PASSWORD}"
      KC_HTTP_PORT: "${KC_HTTP_PORT}"
      KEYCLOAK_ADMIN: "${KEYCLOAK_ADMIN}"
      KEYCLOAK_ADMIN_PASSWORD: "${KEYCLOAK_ADMIN_PASSWORD}"
      KC_HOSTNAME: "localhost"
    ports: ["${KC_HTTP_PORT}:${KC_HTTP_PORT}"]
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:${KC_HTTP_PORT}/health/ready || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 30
    profiles: ["", "persist"]

  keycloak_persist:
    image: "quay.io/keycloak/keycloak:${KEYCLOAK_VERSION}"
    container_name: keycloak
    networks:
      - iron-stack-net
    command: ["start-dev"]
    depends_on:
      postgres_persist: { condition: service_started }
    environment:
      KC_DB: postgres
      KC_DB_URL_HOST: postgres
      KC_DB_URL_DATABASE: "${POSTGRES_DB}"
      KC_DB_USERNAME: "${POSTGRES_USER}"
      KC_DB_PASSWORD: "${POSTGRES_PASSWORD}"
      KC_HTTP_PORT: "${KC_HTTP_PORT}"
      KEYCLOAK_ADMIN: "${KEYCLOAK_ADMIN}"
      KEYCLOAK_ADMIN_PASSWORD: "${KEYCLOAK_ADMIN_PASSWORD}"
      KC_HOSTNAME: "localhost"
    ports: ["${KC_HTTP_PORT}:${KC_HTTP_PORT}"]
    volumes:
      - ./data/keycloak:/opt/keycloak/data:Z
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:${KC_HTTP_PORT}/health/ready || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 30
    profiles: ["persist"]

  # ---------- Elasticsearch ----------
  es:
    image: "docker.elastic.co/elasticsearch/elasticsearch:${ES_VERSION}"
    container_name: es
    networks:
      - iron-stack-net
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - bootstrap.memory_lock=true
      - ES_JAVA_OPTS=-Xms${ES_JAVA_MEM} -Xmx${ES_JAVA_MEM}
    ulimits:
      memlock: { soft: -1, hard: -1 }
      nofile:  { soft: 65536, hard: 65536 }
    ports: ["9200:9200"]
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:9200 >/dev/null || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 40
    profiles: ["", "persist"]
    
  # ---------- Kibana ----------
  kibana:
    image: "docker.elastic.co/kibana/kibana:${ES_VERSION}"
    container_name: kibana
    depends_on:
      es: { condition: service_healthy }
    environment:
      - ELASTICSEARCH_HOSTS=http://es:9200
    ports: ["5601:5601"]
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:5601/api/status || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 20
    profiles: ["", "persist"]

  es_persist:
    image: "docker.elastic.co/elasticsearch/elasticsearch:${ES_VERSION}"
    container_name: es
    networks:
      - iron-stack-net
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - bootstrap.memory_lock=true
      - ES_JAVA_OPTS=-Xms${ES_JAVA_MEM} -Xmx${ES_JAVA_MEM}
    ulimits:
      memlock: { soft: -1, hard: -1 }
      nofile:  { soft: 65536, hard: 65536 }
    ports: ["9200:9200"]
    volumes:
      - ./data/elasticsearch:/usr/share/elasticsearch/data:Z
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:9200 >/dev/null || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 40
    profiles: ["persist"]
    
  kibana_persist:
    image: "docker.elastic.co/kibana/kibana:${ES_VERSION}"
    container_name: kibana
    depends_on:
      es_persist: { condition: service_healthy }
    environment:
      - ELASTICSEARCH_HOSTS=http://es:9200
    ports: ["5601:5601"]
    volumes:
      - ./data/kibana:/usr/share/kibana/data:Z
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:5601/api/status || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 20
    profiles: ["persist"]

  # ---------- Prometheus (ALWAYS persistent) ----------
  prometheus:
    image: "docker.io/prom/prometheus:v${PROM_VERSION}"
    container_name: prometheus
    networks:
      - iron-stack-net
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.enable-lifecycle"
    ports: ["9090:9090"]
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:Z
      - ./data/prometheus:/prometheus:Z
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:9090/-/ready >/dev/null || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 30

  # ---------- Grafana (ALWAYS persistent) ----------
  grafana:
    image: "docker.io/grafana/grafana:${GRAFANA_VERSION}"
    container_name: grafana
    networks:
      - iron-stack-net
    environment:
      GF_SECURITY_ADMIN_USER: "${GRAFANA_ADMIN_USER}"
      GF_SECURITY_ADMIN_PASSWORD: "${GRAFANA_ADMIN_PASSWORD}"
      GF_INSTALL_PLUGINS: ""
    depends_on:
      prometheus: { condition: service_started }
    ports: ["3000:3000"]
    volumes:
      - ./data/grafana:/var/lib/grafana:Z
      - ./provisioning/grafana/datasources:/etc/grafana/provisioning/datasources:Z
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:3000/api/health | grep -q okay || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 30

